{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Analysis\n",
    "The MNIST data files can be downloaded from [this website](http://yann.lecun.com/exdb/mnist/). One little detail: the data files are in the `.idx` format. Fortunately, there is a Python utility we can use to convert the data to `numpy` arrays: the `idx2numpy` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import idx2numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = idx2numpy.convert_from_file('./data/train-images-idx3-ubyte/train-images.idx3-ubyte')\n",
    "train_labels = idx2numpy.convert_from_file('./data/train-labels-idx1-ubyte/train-labels.idx1-ubyte')\n",
    "test_images = idx2numpy.convert_from_file('./data/t10k-images-idx3-ubyte/t10k-images.idx3-ubyte')\n",
    "test_labels = idx2numpy.convert_from_file('./data/t10k-labels-idx1-ubyte/t10k-labels.idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train_images = train_images/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning the labels into NN outputs\n",
    "train_labels_vec = np.zeros((len(train_labels), 10))\n",
    "for i, label in enumerate(train_labels):\n",
    "    train_labels_vec[i, label] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I want to be able to visualize the images\n",
    "i = 20\n",
    "plt.imshow(test_images[i], cmap='gray_r')\n",
    "plt.title(f\"Label: {test_labels[i]}\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can start defining a model. In the MNIST website, there is a table with different models and scores. Let's use one of them: the 2-layer NN, 300 hidden units, mean square error. The reported test error rate of 4.9%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"MNIST_classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " hidden (Dense)              (None, 300)               235500    \n",
      "                                                                 \n",
      " out (Dense)                 (None, 10)                3010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 238,510\n",
      "Trainable params: 238,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = keras.Input(shape=(28,28), name='Input')\n",
    "flatten_layer = keras.layers.Flatten(name='Flatten')\n",
    "hidden_layer = keras.layers.Dense(300, activation='relu', name='hidden')\n",
    "out_layer = keras.layers.Dense(10, name='out')\n",
    "model = keras.Sequential([input_layer, flatten_layer, hidden_layer, out_layer], name='MNIST_classifier')\n",
    "model.summary()\n",
    "model.compile(\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the model on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.0174\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.0096\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.0081\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.0072\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.0066\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.0061\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.0057\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.0054\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.0051\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.0049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d18d6d1c40>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=norm_train_images,\n",
    "    y=train_labels_vec,\n",
    "    batch_size=30,\n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to make some predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_prediction(model, image):\n",
    "    model_raw_output = model(np.array([image])).numpy()\n",
    "    normalized_output = (model_raw_output - np.amin(model_raw_output))/(np.amax(model_raw_output) - np.amin(model_raw_output))\n",
    "    predicted_value = np.argmax(normalized_output)\n",
    "    return predicted_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.978\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for test_image, test_label in zip(test_images, test_labels):\n",
    "    if model_prediction(model, test_image) == test_label:\n",
    "        count += 1\n",
    "\n",
    "acc = count/len(test_labels)\n",
    "print(f'Test accuracy: {acc:.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good! Can we improve this result? Let's train some more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.0047\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0046\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0044\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0042\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.0041\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.0040\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.0039\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.0038\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.0037\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0036\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.0035\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.0035\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0034\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0033\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0032\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.0032\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.0031\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0031\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0030\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.0030\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.0030\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.0029\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.0029\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0028\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0028\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0028\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0027\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0027\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0026\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d18d6d3a00>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=norm_train_images,\n",
    "    y=train_labels_vec,\n",
    "    batch_size=30,\n",
    "    epochs=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.974\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for test_image, test_label in zip(test_images, test_labels):\n",
    "    if model_prediction(model, test_image) == test_label:\n",
    "        count += 1\n",
    "\n",
    "acc = count/len(test_labels)\n",
    "print(f'Test accuracy: {acc:.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further training actually WORSENED our results! This is a clear sign of overfitting. Let's train some more and see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.0026\n",
      "Epoch 2/60\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0026\n",
      "Epoch 3/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0025\n",
      "Epoch 4/60\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.0025\n",
      "Epoch 5/60\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.0025\n",
      "Epoch 6/60\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0025\n",
      "Epoch 7/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0024\n",
      "Epoch 8/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0024\n",
      "Epoch 9/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0024\n",
      "Epoch 10/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0024\n",
      "Epoch 11/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0024\n",
      "Epoch 12/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0023\n",
      "Epoch 13/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0023\n",
      "Epoch 14/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0023\n",
      "Epoch 15/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0023\n",
      "Epoch 16/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0023\n",
      "Epoch 17/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0022\n",
      "Epoch 18/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0022\n",
      "Epoch 19/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0022\n",
      "Epoch 20/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0022\n",
      "Epoch 21/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0022\n",
      "Epoch 22/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0021\n",
      "Epoch 23/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0021\n",
      "Epoch 24/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0021\n",
      "Epoch 25/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0021\n",
      "Epoch 26/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0021\n",
      "Epoch 27/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0021\n",
      "Epoch 28/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0021\n",
      "Epoch 29/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0021\n",
      "Epoch 30/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0020\n",
      "Epoch 31/60\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.0020\n",
      "Epoch 32/60\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.0020\n",
      "Epoch 33/60\n",
      "2000/2000 [==============================] - 9s 5ms/step - loss: 0.0020\n",
      "Epoch 34/60\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.0020\n",
      "Epoch 35/60\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.0020\n",
      "Epoch 36/60\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0020\n",
      "Epoch 37/60\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0019\n",
      "Epoch 38/60\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0019\n",
      "Epoch 39/60\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0019\n",
      "Epoch 40/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0019\n",
      "Epoch 41/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0019\n",
      "Epoch 42/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0019\n",
      "Epoch 43/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0019\n",
      "Epoch 44/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0019\n",
      "Epoch 45/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0019\n",
      "Epoch 46/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0019\n",
      "Epoch 47/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0018\n",
      "Epoch 48/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0018\n",
      "Epoch 49/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0018\n",
      "Epoch 50/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0018\n",
      "Epoch 51/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0018\n",
      "Epoch 52/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0018\n",
      "Epoch 53/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0018\n",
      "Epoch 54/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0018\n",
      "Epoch 55/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0018\n",
      "Epoch 56/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0018\n",
      "Epoch 57/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0018\n",
      "Epoch 58/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0018\n",
      "Epoch 59/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0017\n",
      "Epoch 60/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d18d6d3bb0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=norm_train_images,\n",
    "    y=train_labels_vec,\n",
    "    batch_size=30,\n",
    "    epochs=60,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.954\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for test_image, test_label in zip(test_images, test_labels):\n",
    "    if model_prediction(model, test_image) == test_label:\n",
    "        count += 1\n",
    "\n",
    "acc = count/len(test_labels)\n",
    "print(f'Test accuracy: {acc:.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly overfitting! How do we prevent overfitting? Regularization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"MNIST_classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " hidden (Dense)              (None, 300)               235500    \n",
      "                                                                 \n",
      " out (Dense)                 (None, 10)                3010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 238,510\n",
      "Trainable params: 238,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = keras.Input(shape=(28,28), name='Input')\n",
    "flatten_layer = keras.layers.Flatten(name='Flatten')\n",
    "hidden_layer = keras.layers.Dense(300, activation='relu', kernel_regularizer=keras.regularizers.l1_l2(l1=1e-5, l2=9e-5), name='hidden')\n",
    "out_layer = keras.layers.Dense(10, name='out')\n",
    "model = keras.Sequential([input_layer, flatten_layer, hidden_layer, out_layer], name='MNIST_classifier')\n",
    "model.summary()\n",
    "model.compile(\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do some learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_decay(epoch, learning_rate):\n",
    "    if epoch != 0 and epoch % 8 == 0:\n",
    "        return learning_rate/3\n",
    "    else:\n",
    "        return learning_rate\n",
    "learning_scheduler = keras.callbacks.LearningRateScheduler(learning_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.0109 - lr: 1.2346e-05\n",
      "Epoch 2/15\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0109 - lr: 1.2346e-05\n",
      "Epoch 3/15\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0109 - lr: 1.2346e-05\n",
      "Epoch 4/15\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.0109 - lr: 1.2346e-05\n",
      "Epoch 5/15\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.0109 - lr: 1.2346e-05\n",
      "Epoch 6/15\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.0109 - lr: 1.2346e-05\n",
      "Epoch 7/15\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.0109 - lr: 1.2346e-05\n",
      "Epoch 8/15\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.0109 - lr: 1.2346e-05\n",
      "Epoch 9/15\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.0108 - lr: 4.1152e-06\n",
      "Epoch 10/15\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.0108 - lr: 4.1152e-06\n",
      "Epoch 11/15\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.0108 - lr: 4.1152e-06\n",
      "Epoch 12/15\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.0108 - lr: 4.1152e-06\n",
      "Epoch 13/15\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.0108 - lr: 4.1152e-06\n",
      "Epoch 14/15\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.0108 - lr: 4.1152e-06\n",
      "Epoch 15/15\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.0108 - lr: 4.1152e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d1842bc1f0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=norm_train_images,\n",
    "    y=train_labels_vec,\n",
    "    batch_size=30,\n",
    "    epochs=15,\n",
    "    callbacks=[learning_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.934\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for test_image, test_label in zip(test_images, test_labels):\n",
    "    if model_prediction(model, test_image) == test_label:\n",
    "        count += 1\n",
    "\n",
    "acc = count/len(test_labels)\n",
    "print(f'Test accuracy: {acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cocoa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
